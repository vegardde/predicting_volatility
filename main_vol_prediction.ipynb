{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility Prediction in Financial Markets  - Model Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sci\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
    "from scipy import stats\n",
    "import gc\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train_X = pd.read_csv('data/training_input.csv', sep=';')\n",
    "train_y = pd.read_csv('data/training_output.csv', sep=';')\n",
    "orig_test_X  = pd.read_csv('data/testing_input.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate number of and Impute missing values via interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility_cols = [c for c in orig_train_X.columns if c.startswith('volatility')]\n",
    "return_cols = [c for c in orig_train_X.columns if c.startswith('return')]\n",
    "other_cols = ['date' , 'product_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = orig_train_X.drop(orig_train_X[return_cols],axis=1)\n",
    "train_R = orig_train_X.drop(orig_train_X[volatility_cols], axis=1)\n",
    "test_X  = orig_test_X.drop(orig_test_X[return_cols], axis=1)\n",
    "test_R  = orig_test_X.drop(orig_test_X[volatility_cols], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count nans and zeros i.e. no price change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_NANs = pd.DataFrame({'ID': orig_train_X['ID'],'product_id': orig_train_X['product_id'],'num_NANs':(orig_train_X[volatility_cols].isnull()).sum(axis =1).astype(dtype = 'float64',copy=False)})\n",
    "train_true_zeros = pd.DataFrame({'ID': orig_train_X['ID'],'product_id': orig_train_X['product_id'],'true_zeros': (orig_train_X[volatility_cols]==0).sum(axis =1).astype(dtype = 'float64',copy=False)})\n",
    "test_num_NANs = pd.DataFrame({'ID': orig_test_X['ID'],'product_id': orig_test_X['product_id'],'num_NANs':(orig_test_X[volatility_cols].isnull()).sum(axis =1).astype(dtype = 'float64',copy=False)})\n",
    "test_true_zeros = pd.DataFrame({'ID': orig_test_X['ID'],'product_id': orig_test_X['product_id'],'true_zeros': (orig_test_X[volatility_cols]==0).sum(axis =1).astype(dtype = 'float64',copy=False)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We linearly interpolate the NaNs corresponding to volatilities and set NaN returns to 0 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default interpolation direction is forward\n",
    "train_X.iloc[:,3:] = train_X.iloc[:,3:].interpolate(axis=1)\n",
    "test_X.iloc[:,3:]  = test_X.iloc[:,3:].interpolate(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.fillna(0, inplace=True) \n",
    "test_X.fillna(0, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_R.fillna(0, inplace=True) \n",
    "test_R.fillna(0, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features =train_X.drop(train_X[volatility_cols],axis =1)\n",
    "train_features['min_vol']    = np.min(train_X.iloc[:,3:], axis=1)\n",
    "train_features['max_vol']    = np.max(train_X.iloc[:,3:], axis=1)\n",
    "train_features['std_vol']    = np.std(train_X.iloc[:,3:], axis=1)\n",
    "train_features['median_vol'] = np.median(train_X.iloc[:,3:], axis=1)\n",
    "train_features['mean_vol'] = np.mean(train_X.iloc[:,3:], axis=1)\n",
    "train_features['skew_vol'] = stats.skew(train_X.iloc[:,3:], axis=1)\n",
    "train_features['kurtosis_vol'] = stats.kurtosis(train_X.iloc[:,3:], axis=1)\n",
    "bin_length = 9\n",
    "train_features['bin1_vol']  = np.mean(train_X.iloc[:,3:3+bin_length-1], axis=1)\n",
    "train_features['bin2_vol']  = np.mean(train_X.iloc[:,3+bin_length:3+2*bin_length-1], axis=1)\n",
    "train_features['bin3_vol']  = np.mean(train_X.iloc[:,3+2*bin_length:3+3*bin_length-1], axis=1)\n",
    "train_features['bin4_vol']  = np.mean(train_X.iloc[:,3+3*bin_length:3+4*bin_length-1], axis=1)\n",
    "train_features['bin5_vol']  = np.mean(train_X.iloc[:,3+4*bin_length:3+5*bin_length-1], axis=1)\n",
    "train_features['bin6_vol']  = np.mean(train_X.iloc[:,3+5*bin_length:3+6*bin_length-1], axis=1)\n",
    "#train_features['num_NANs'] =  train_num_NANs ## bytt ut med per produkt\n",
    "#train_features['true_zeros'] =  train_true_zeros ## bytt ut med per produkt\n",
    "\n",
    "###Differencing to account for ACF feature\n",
    "train_features['lagged_diff_mean']= np.mean(np.diff(train_X.iloc[:,3:], axis=1),axis=1)\n",
    "train_features['lagged_diff_std']= np.std(np.diff(train_X.iloc[:,3:], axis=1),axis=1)\n",
    "train_features['lagged_diff_max']= np.max(np.diff(train_X.iloc[:,3:], axis=1),axis=1)\n",
    "train_features['lagged_diff_min']= np.min(np.diff(train_X.iloc[:,3:], axis=1),axis=1)\n",
    "\n",
    "### Date Spesific features####\n",
    "date_mean = pd.DataFrame({'date': train_X['date'].unique(),'date_mean': np.array(train_X.groupby('date')[volatility_cols].mean().mean(axis=1))})\n",
    "train_features = train_features.merge(date_mean,on='date',how = \"left\").set_index(train_features.index)\n",
    "\n",
    "train_features = train_features.merge(train_X.groupby('date')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].mean(),on='date',how = \"left\").set_index(train_features.index)\n",
    "train_features.rename(columns={'volatility 13:45:00': '13:45_mean_vol_date', 'volatility 13:50:00': '13:50_mean_vol_date','volatility 13:55:00':'13:55_mean_vol_date'}, inplace=True)\n",
    "\n",
    "train_features = train_features.merge(train_X.groupby('date')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].std(),on='date',how = \"left\").set_index(train_features.index)\n",
    "train_features.rename(columns={'volatility 13:45:00': '13:45_std_vol_date', 'volatility 13:50:00': '13:50_std_vol_date','volatility 13:55:00':'13:55_std_vol_date'}, inplace=True)\n",
    "\n",
    "train_features = train_features.merge(train_X.groupby('date')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].max(),on='date',how = \"left\").set_index(train_features.index)\n",
    "train_features.rename(columns={'volatility 13:45:00': '13:45_max_vol_date', 'volatility 13:50:00': '13:50_max_vol_date','volatility 13:55:00':'13:55_max_vol_date'}, inplace=True)\n",
    "\n",
    "train_features = train_features.merge(train_X.groupby('date')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].min(),on='date',how = \"left\").set_index(train_features.index)\n",
    "train_features.rename(columns={'volatility 13:45:00': '13:45_min_vol_date', 'volatility 13:50:00': '13:50_min_vol_date','volatility 13:55:00':'13:55_min_vol_date'}, inplace=True)\n",
    "\n",
    "train_features = train_features.merge(train_X.groupby('date')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].skew(),on='date',how = \"left\").set_index(train_features.index)\n",
    "train_features.rename(columns={'volatility 13:45:00': '13:45_skew_vol_date', 'volatility 13:50:00': '13:50_skew_vol_date','volatility 13:55:00':'13:55_skew_vol_date'}, inplace=True)\n",
    "\n",
    "train_features = train_features.merge(train_X.groupby('date')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].apply(pd.DataFrame.kurt),on='date',how = \"left\").set_index(train_features.index)\n",
    "train_features.rename(columns={'volatility 13:45:00': '13:45_kurt_vol_date', 'volatility 13:50:00': '13:50_kurt_vol_date','volatility 13:55:00':'13:55_kurt_vol_date'}, inplace=True)\n",
    "\n",
    "# #### Product spesific features#####\n",
    "\n",
    "product_mean = pd.DataFrame({'product_id': train_X['product_id'].unique(),'product_mean': np.array(train_X.groupby('product_id')[volatility_cols].mean().mean(axis=1))})\n",
    "train_features = train_features.merge(product_mean,on='product_id',how = \"left\").set_index(train_features.index)\n",
    "\n",
    "train_features = train_features.merge(train_X.groupby('product_id')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].mean(),on='product_id',how = \"left\").set_index(train_features.index)\n",
    "train_features.rename(columns={'volatility 13:45:00': '13:45_mean_vol_stock', 'volatility 13:50:00': '13:50_mean_vol_stock','volatility 13:55:00':'13:55_mean_vol_stock'}, inplace=True)\n",
    "\n",
    "train_features = train_features.merge(train_X.groupby('product_id')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].std(),on='product_id',how = \"left\").set_index(train_features.index)\n",
    "train_features.rename(columns={'volatility 13:45:00': '13:45_std_vol_stock', 'volatility 13:50:00': '13:50_std_vol_stock','volatility 13:55:00':'13:55_std_vol_stock'}, inplace=True)\n",
    "\n",
    "train_features = train_features.merge(train_X.groupby('product_id')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].max(),on='product_id',how = \"left\").set_index(train_features.index)\n",
    "train_features.rename(columns={'volatility 13:45:00': '13:45_max_vol_stock', 'volatility 13:50:00': '13:50_max_vol_stock','volatility 13:55:00':'13:55_max_vol_stock'}, inplace=True)\n",
    "\n",
    "train_features = train_features.merge(train_X.groupby('product_id')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].min(),on='product_id',how = \"left\").set_index(train_features.index)\n",
    "train_features.rename(columns={'volatility 13:45:00': '13:45_min_vol_stock', 'volatility 13:50:00': '13:50_min_vol_stock','volatility 13:55:00':'13:55_min_vol_stock'}, inplace=True)\n",
    "\n",
    "### Return features\n",
    "train_features['ret_sign_std'] = np.var(train_R.iloc[:,3:], axis=1)\n",
    "train_features['ret_sign_accum'] = np.sum(train_R.iloc[:,3:], axis=1)\n",
    "\n",
    "#Merge zeros onto data frame\n",
    "train_features = train_features.merge(train_num_NANs.drop(columns='product_id'), on='ID',how = \"left\").set_index(train_features.index)\n",
    "train_features = train_features.merge(train_true_zeros.drop(columns='product_id'), on='ID',how = \"left\").set_index(train_features.index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merge target onto data frame\n",
    "train_features = train_features.merge(train_y, on='ID',how = \"left\").set_index(train_features.index)\n",
    "\n",
    "train_features.fillna(0, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features =test_X.drop(test_X[volatility_cols],axis =1)\n",
    "test_features['min_vol']    = np.min(test_X.iloc[:,3:], axis=1)\n",
    "test_features['max_vol']    = np.max(test_X.iloc[:,3:], axis=1)\n",
    "test_features['std_vol']    = np.std(test_X.iloc[:,3:], axis=1)\n",
    "test_features['median_vol'] = np.median(test_X.iloc[:,3:], axis=1)\n",
    "test_features['mean_vol'] = np.mean(test_X.iloc[:,3:], axis=1)\n",
    "test_features['skew_vol'] = stats.skew(test_X.iloc[:,3:], axis=1)\n",
    "test_features['kurtosis_vol'] = stats.kurtosis(test_X.iloc[:,3:], axis=1)\n",
    "bin_length = 9\n",
    "test_features['bin1_vol']  = np.mean(test_X.iloc[:,3:3+bin_length-1], axis=1)\n",
    "test_features['bin2_vol']  = np.mean(test_X.iloc[:,3+bin_length:3+2*bin_length-1], axis=1)\n",
    "test_features['bin3_vol']  = np.mean(test_X.iloc[:,3+2*bin_length:3+3*bin_length-1], axis=1)\n",
    "test_features['bin4_vol']  = np.mean(test_X.iloc[:,3+3*bin_length:3+4*bin_length-1], axis=1)\n",
    "test_features['bin5_vol']  = np.mean(test_X.iloc[:,3+4*bin_length:3+5*bin_length-1], axis=1)\n",
    "test_features['bin6_vol']  = np.mean(test_X.iloc[:,3+5*bin_length:3+6*bin_length-1], axis=1)\n",
    "#test_features['num_NANs'] =  test_num_NANs ## bytt ut med per produkt\n",
    "#test_features['true_zeros'] =  test_true_zeros ## bytt ut med per produkt\n",
    "\n",
    "###Differencing to account for ACF feature\n",
    "test_features['lagged_diff_mean']= np.mean(np.diff(test_X.iloc[:,3:], axis=1),axis=1)\n",
    "test_features['lagged_diff_std']= np.std(np.diff(test_X.iloc[:,3:], axis=1),axis=1)\n",
    "test_features['lagged_diff_max']= np.max(np.diff(test_X.iloc[:,3:], axis=1),axis=1)\n",
    "test_features['lagged_diff_min']= np.min(np.diff(test_X.iloc[:,3:], axis=1),axis=1)\n",
    "\n",
    "### Date Spesific features####\n",
    "date_mean = pd.DataFrame({'date': test_X['date'].unique(),'date_mean': np.array(test_X.groupby('date')[volatility_cols].mean().mean(axis=1))})\n",
    "test_features = test_features.merge(date_mean,on='date',how = \"left\").set_index(test_features.index)\n",
    "\n",
    "test_features = test_features.merge(test_X.groupby('date')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].mean(),on='date',how = \"left\").set_index(test_features.index)\n",
    "test_features.rename(columns={'volatility 13:45:00': '13:45_mean_vol_date', 'volatility 13:50:00': '13:50_mean_vol_date','volatility 13:55:00':'13:55_mean_vol_date'}, inplace=True)\n",
    "\n",
    "test_features = test_features.merge(test_X.groupby('date')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].std(),on='date',how = \"left\").set_index(test_features.index)\n",
    "test_features.rename(columns={'volatility 13:45:00': '13:45_std_vol_date', 'volatility 13:50:00': '13:50_std_vol_date','volatility 13:55:00':'13:55_std_vol_date'}, inplace=True)\n",
    "\n",
    "test_features = test_features.merge(test_X.groupby('date')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].max(),on='date',how = \"left\").set_index(test_features.index)\n",
    "test_features.rename(columns={'volatility 13:45:00': '13:45_max_vol_date', 'volatility 13:50:00': '13:50_max_vol_date','volatility 13:55:00':'13:55_max_vol_date'}, inplace=True)\n",
    "\n",
    "test_features = test_features.merge(test_X.groupby('date')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].min(),on='date',how = \"left\").set_index(test_features.index)\n",
    "test_features.rename(columns={'volatility 13:45:00': '13:45_min_vol_date', 'volatility 13:50:00': '13:50_min_vol_date','volatility 13:55:00':'13:55_min_vol_date'}, inplace=True)\n",
    "\n",
    "test_features = test_features.merge(test_X.groupby('date')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].skew(),on='date',how = \"left\").set_index(test_features.index)\n",
    "test_features.rename(columns={'volatility 13:45:00': '13:45_skew_vol_date', 'volatility 13:50:00': '13:50_skew_vol_date','volatility 13:55:00':'13:55_skew_vol_date'}, inplace=True)\n",
    "\n",
    "test_features = test_features.merge(test_X.groupby('date')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].apply(pd.DataFrame.kurt),on='date',how = \"left\").set_index(test_features.index)\n",
    "test_features.rename(columns={'volatility 13:45:00': '13:45_kurt_vol_date', 'volatility 13:50:00': '13:50_kurt_vol_date','volatility 13:55:00':'13:55_kurt_vol_date'}, inplace=True)\n",
    "\n",
    "# #### Product spesific features#####\n",
    "\n",
    "product_mean = pd.DataFrame({'product_id': test_X['product_id'].unique(),'product_mean': np.array(test_X.groupby('product_id')[volatility_cols].mean().mean(axis=1))})\n",
    "test_features = test_features.merge(product_mean,on='product_id',how = \"left\").set_index(test_features.index)\n",
    "\n",
    "test_features = test_features.merge(test_X.groupby('product_id')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].mean(),on='product_id',how = \"left\").set_index(test_features.index)\n",
    "test_features.rename(columns={'volatility 13:45:00': '13:45_mean_vol_stock', 'volatility 13:50:00': '13:50_mean_vol_stock','volatility 13:55:00':'13:55_mean_vol_stock'}, inplace=True)\n",
    "\n",
    "test_features = test_features.merge(test_X.groupby('product_id')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].std(),on='product_id',how = \"left\").set_index(test_features.index)\n",
    "test_features.rename(columns={'volatility 13:45:00': '13:45_std_vol_stock', 'volatility 13:50:00': '13:50_std_vol_stock','volatility 13:55:00':'13:55_std_vol_stock'}, inplace=True)\n",
    "\n",
    "test_features = test_features.merge(test_X.groupby('product_id')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].max(),on='product_id',how = \"left\").set_index(test_features.index)\n",
    "test_features.rename(columns={'volatility 13:45:00': '13:45_max_vol_stock', 'volatility 13:50:00': '13:50_max_vol_stock','volatility 13:55:00':'13:55_max_vol_stock'}, inplace=True)\n",
    "\n",
    "test_features = test_features.merge(test_X.groupby('product_id')['volatility 13:45:00','volatility 13:50:00','volatility 13:55:00'].min(),on='product_id',how = \"left\").set_index(test_features.index)\n",
    "test_features.rename(columns={'volatility 13:45:00': '13:45_min_vol_stock', 'volatility 13:50:00': '13:50_min_vol_stock','volatility 13:55:00':'13:55_min_vol_stock'}, inplace=True)\n",
    "\n",
    "### Return features\n",
    "test_features['ret_sign_std'] = np.var(test_R.iloc[:,3:], axis=1)\n",
    "test_features['ret_sign_accum'] = np.sum(test_R.iloc[:,3:], axis=1)\n",
    "\n",
    "\n",
    "#Merge zeros onto data frame\n",
    "test_features = test_features.merge(test_num_NANs.drop(columns='product_id'), on='ID',how = \"left\").set_index(test_features.index)\n",
    "test_features = test_features.merge(test_true_zeros.drop(columns='product_id'), on='ID',how = \"left\").set_index(test_features.index)\n",
    "\n",
    "test_features.fillna(0, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define error function (mean average percent error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE_per_obs(y_true, y_pred):\n",
    "    return (np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide benchmark models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_mean = train_X_[['mean_vol','TARGET']]\n",
    "val_pred_mean = val_X_[['mean_vol','TARGET']]\n",
    "\n",
    "print('Train error=', round(MAPE(train_features['TARGET'], train_features['mean_vol']), 4), '%')\n",
    "print('Validation error =', round(MAPE(val_pred_mean['TARGET'], val_pred_mean['mean_vol']), 4), '%')\n",
    "\n",
    "test_pred_mean = test_features[['ID','mean_vol']]\n",
    "test_pred_mean = test_pred_mean.rename(columns = {'mean_vol':'TARGET'})\n",
    "test_pred_mean.to_csv('results/mean_pred.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_median = train_X_[['median_vol','TARGET']]\n",
    "val_pred_median = val_X_[['median_vol','TARGET']]\n",
    "\n",
    "print('Train error=', round(MAPE(train_features['TARGET'], train_features['median_vol']), 4), '%')\n",
    "print('Validation error =', round(MAPE(val_pred_median['TARGET'], val_pred_median['median_vol']), 4), '%')\n",
    "test_pred_median = test_features[['ID','median_vol']]\n",
    "test_pred_median = test_pred_median.rename(columns = {'median_vol':'TARGET'})\n",
    "test_pred_median.to_csv('results/median_pred.csv', sep=';', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nomalise data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "#scaler = RobustScaler(quantile_range=(40, 60))\n",
    "#scaler = PowerTransformer(method = 'box-cox')\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "train_X_norm = train_X_.copy()\n",
    "val_X_norm = val_X_.copy()\n",
    "train_features_norm =train_features.copy()\n",
    "test_features_norm =test_features.copy()\n",
    "\n",
    "\n",
    "#Scale train and validation data\n",
    "transformer_validation = scaler.fit(train_X_[regression_cols])\n",
    "train_X_norm[regression_cols] = transformer_validation.transform(train_X_norm[regression_cols])\n",
    "val_X_norm[regression_cols] = transformer_validation.transform(val_X_norm[regression_cols])\n",
    "\n",
    "#Scale train and test data\n",
    "transformer = scaler.fit(train_features[regression_cols])\n",
    "train_features_norm[regression_cols] = transformer.transform(train_features_norm[regression_cols])\n",
    "test_features_norm[regression_cols] = transformer.transform(test_features_norm[regression_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training global regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "def score_function(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "# wraps scoring functions for use in GridSearchCV and cross_val_score.\n",
    "mape = make_scorer(score_function, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    #'alpha':[0.00006, 0.00007, 0.00008, 0.00009, 0.00010, 0.00011, 0.00012, 0.00013], \n",
    "    'alpha':10.0**-np.arange(4,7),\n",
    "    #'alpha': [0.00005,0.0001],\n",
    "    'l1_ratio': [0.1,0.3,0.5,0.7,.9,1.0],\n",
    "    #'l1_ratio': [0.0],\n",
    "    'loss': ['huber'],\n",
    "    'max_iter': [10000000],\n",
    "    'penalty': ['elasticnet'],\n",
    "    'early_stopping': [False],\n",
    "    'tol': [0.00000001],\n",
    "    'fit_intercept':[True],\n",
    "    #'epsilon': [0.0005,0.001,0.005,0.01,0.03,0.06],\n",
    "    'epsilon': [0.001,0.005,0.01,0.015],\n",
    "    'shuffle': [False],\n",
    "    'learning_rate': ['optimal'],\n",
    "    'eta0': [0.5],\n",
    "    'power_t': [0.25],\n",
    "    'n_iter_no_change': [5],\n",
    "}\n",
    "paramGrid = ParameterGrid(grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDRegressor()\n",
    "grid = GridSearchCV(estimator=model, param_grid=grid,scoring=mape, n_jobs = -1, cv =5)\n",
    "grid_result = grid.fit(train_features_norm[regression_cols], train_features_norm['TARGET'])\n",
    "\n",
    "best_model =grid_result.best_estimator_\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = best_model.fit(train_features_norm[regression_cols],train_features_norm['TARGET'])\n",
    "fitted_models.append(fitted_model)\n",
    "print('Train error SGD regressor =', round(MAPE(train_features_norm['TARGET'], np.maximum(fitted_model.predict(train_features_norm[regression_cols]),0)), 4), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training local regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    #'alpha':[0.00006, 0.00007, 0.00008, 0.00009, 0.00010, 0.00011, 0.00012, 0.00013], \n",
    "    #'alpha':10.0**-np.arange(2,5),\n",
    "    'alpha': [0.00001,0.0001,0.0005,0.001],\n",
    "    'l1_ratio': [0.05,0.1,0.3,0.5,0.7,.9,1.0],\n",
    "    'loss': ['huber'],\n",
    "    'max_iter': [10000000],\n",
    "    'penalty': ['elasticnet'],\n",
    "    'early_stopping': [False],\n",
    "    'tol': [0.00000001],\n",
    "    'fit_intercept':[True],\n",
    "    'epsilon': [0.0001,0.0005,0.001,0.005,0.01,0.03],\n",
    "    #'epsilon': [0.0001,0.001,0.005,0.01],\n",
    "    'shuffle': [False],\n",
    "    'learning_rate': ['optimal'],\n",
    "    'eta0': [0.5],\n",
    "    'power_t': [0.25],\n",
    "    'n_iter_no_change': [5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDRegressor()\n",
    "grid = GridSearchCV(estimator=model, param_grid=grid,scoring=mape, n_jobs = -1, cv =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModels = []\n",
    "bestScores = [] \n",
    "product_ids = train_X_norm['product_id'].unique()\n",
    "\n",
    "train_pred = pd.DataFrame(columns=['ID','prediction'])\n",
    "test_pred = pd.DataFrame(columns=['ID','TARGET'])\n",
    "\n",
    "for product_id in product_ids:\n",
    "#for product_id in range(1,10):\n",
    "#for product_id in range(165,166):    \n",
    "\n",
    "    df_slice_train = train_features_norm[train_features_norm['product_id'] == product_id]\n",
    "    df_slice_test = test_features_norm[test_features_norm['product_id'] == product_id]\n",
    "    grid_result = grid.fit(df_slice_train[regression_cols], df_slice_train['TARGET'])\n",
    "    \n",
    "    \n",
    "    print(\"Product_id: \",product_id)\n",
    "    print(\"MAPE Score: \",grid_result.best_score_)\n",
    "    bestModels.append(grid_result.best_estimator_)\n",
    "    bestScores.append(grid_result.best_score_)\n",
    "    \n",
    "    train_pred = train_pred.append(pd.DataFrame({'ID': df_slice_train['ID'],'prediction': np.maximum(grid_result.best_estimator_.predict(df_slice_train[regression_cols]),0.0)}))\n",
    "    test_pred = test_pred.append(pd.DataFrame({'ID': df_slice_test['ID'],'TARGET': np.maximum(grid_result.best_estimator_.predict(df_slice_test[regression_cols]),0.0)}))   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred['ID'] = train_pred['ID'].astype(np.int64)\n",
    "temp = train_features_norm.merge(train_pred,on='ID',how = \"left\").set_index(train_features_norm.index)\n",
    "print('Train error SGD regressor =', round(MAPE(temp['TARGET'],temp['prediction']), 4), '%')\n",
    "plt.plot(MAPE_per_obs(temp['TARGET'],temp['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_pred.set_index('ID')\n",
    "test_pred.loc[test_X['ID']].to_csv('results/local_SGD_regressor_new_last.csv', sep=';', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training feed-forward neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "from sklearn.utils import parallel_backend\n",
    "seed = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.utils import plot_model\n",
    "import pydot\n",
    "from keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please ensure you have installed TensorFlow correctly')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define model\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return K.mean(np.abs((y_true - y_pred) / y_true))* 100\n",
    "\n",
    "def baseline_model(l1=30,l2=10, l3=5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(l1, name='Layer_1', input_dim=len(regression_cols), activation='relu',kernel_initializer='normal'))\n",
    "    model.add(Dense(l2, name='Layer_2', activation='relu',kernel_initializer='normal'))\n",
    "    model.add(Dense(l3, name='Layer_3', activation='relu',kernel_initializer='normal'))\n",
    "    model.add(Dense(1,name='Output',activation='relu',kernel_initializer='normal'))\n",
    "   \n",
    "    \n",
    "    \n",
    "    model.compile(loss=custom_loss,optimizer='Adagrad', metrics=[custom_loss])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=20, batch_size=5000,verbose=True)\n",
    "l1 =[50,30,20]\n",
    "l2 = [20,10,5]\n",
    "l3 = [10,5,3]\n",
    "\n",
    "param_grid = dict(l1=l1,l2=l2,l3=l3)\n",
    "grid = GridSearchCV(estimator=estimator, param_grid=param_grid,scoring=mape, n_jobs = 1, cv =5,return_train_score=True)\n",
    "grid_result = grid.fit(train_features_norm[regression_cols], train_features_norm['TARGET'])\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_prediction =np.maximum(grid_result.best_estimator_.predict(train_features_norm[regression_cols]),0.0)\n",
    "print('score train: ', score_function(train_features_norm['TARGET'], nn_train_prediction))\n",
    "\n",
    "nn_test_prediction=np.maximum(grid_result.best_estimator_.predict(test_features_norm[regression_cols]),0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train error neural netowork =', round(MAPE(train_features_norm['TARGET'], nn_train_prediction), 4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_predictions = pd.DataFrame({'ID': test_features_norm['ID'],'TARGET': nn_test_prediction})\n",
    "nn_predictions.to_csv('results/neural_predictions_best.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define model\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return K.mean(np.abs((y_true - y_pred) / y_true))* 100\n",
    "\n",
    "def LSTM_model():\n",
    "    n_steps = 54\n",
    "    n_features =1\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(25,kernel_initializer='normal',input_shape=(n_steps, n_features)))\n",
    " #   model.add(LSTM(50, activation='relu',kernel_initializer='normal', return_sequences=True,input_shape=(n_steps, n_features)))\n",
    "#    model.add(LSTM(10, activation='relu'))\n",
    "   # model.add(Dense(10,activation='relu',kernel_initializer='normal'))\n",
    "    model.add(Dense(1,name='Output',activation='tanh',kernel_initializer='normal'))\n",
    "       \n",
    "    \n",
    "    model.compile(loss=custom_loss,optimizer='Adagrad', metrics=[custom_loss])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting the data reshaped to correct LSTM input structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lstm = np.array(train_X.iloc[:,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lstm_test = np.array(test_X.iloc[:,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lstm =np.array(train_y.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lstm = X_lstm.reshape((X_lstm.shape[0], X_lstm.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lstm_test = X_lstm_test.reshape((X_lstm_test.shape[0], X_lstm_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting the LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_estimator = KerasRegressor(build_fn=LSTM_model, epochs=25, batch_size=5000,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_estimator.fit(X_lstm,y_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train_preds = lstm_estimator.predict(X_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train error neural netowork =', round(MAPE(train_y['TARGET'], lstm_train_preds), 4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_test_preds = lstm_estimator.predict(X_lstm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predictions = pd.DataFrame({'ID': test_X['ID'],'TARGET': lstm_test_preds})\n",
    "lstm_predictions.to_csv('results/lstm_regressor_25.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
